{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 참고한 코드: https://bumcrush.tistory.com/116 [맑음때때로 여름]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n",
      "151\n",
      "161\n",
      "171\n",
      "181\n",
      "191\n",
      "201\n",
      "211\n",
      "221\n",
      "231\n",
      "241\n",
      "251\n",
      "261\n",
      "271\n",
      "281\n",
      "291\n",
      "301\n",
      "311\n",
      "321\n",
      "331\n",
      "341\n",
      "351\n",
      "361\n",
      "371\n",
      "381\n",
      "391\n",
      "401\n",
      "411\n",
      "421\n",
      "431\n",
      "441\n",
      "451\n",
      "461\n",
      "471\n",
      "481\n",
      "491\n",
      "501\n",
      "511\n",
      "521\n",
      "531\n",
      "541\n",
      "551\n",
      "561\n",
      "571\n",
      "581\n",
      "591\n",
      "601\n",
      "611\n",
      "621\n",
      "631\n",
      "641\n",
      "651\n",
      "661\n",
      "671\n",
      "681\n",
      "691\n",
      "701\n",
      "711\n",
      "721\n",
      "731\n",
      "741\n",
      "751\n",
      "761\n",
      "771\n",
      "781\n",
      "791\n",
      "801\n",
      "811\n",
      "821\n",
      "831\n",
      "841\n",
      "851\n",
      "861\n",
      "871\n",
      "881\n"
     ]
    }
   ],
   "source": [
    "result = crawler(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('20200101_20200930_stalking_naver.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(keyword, maxpage, startdate, enddate):\n",
    "    '''\n",
    "    keyword: (string) 검색어\n",
    "    maxpage: (int) 크롤링할 총 페이지 수\n",
    "    startdate: (string)(2020.01.01) 검색할 시작 날짜\n",
    "    enddate: (string)(2020.01.31) 검색할 마지막 날짜\n",
    "    '''\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1 # 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    final_result = pd.DataFrame(columns=['date', 'title', 'source', 'contents', 'link'])\n",
    "    while page <= maxpage_t:\n",
    "        # field=1 제목 photo=3 지면기사 \n",
    "        # sort=1 최신순 sort=2 오래된순\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\"\\\n",
    "              + keword\\\n",
    "              + \"&sm=tab_pge&sort=2&photo=0&field=1&reporter_article=&pd=3&ds=\"\\\n",
    "              + startdate\\\n",
    "              + \"&de=\"\\\n",
    "              + enddate\\\n",
    "              + \"&docid=&nso=so:dd,p:from\"\\\n",
    "              + startdate.replace('.', '')\\\n",
    "              + \"to\"\\\n",
    "              + enddate.replace('.', '')\\\n",
    "              +\",a:t&mynews=0&start=\" + str(page) + \"&refresh_start=0\"\n",
    "        response = requests.get(url)\n",
    "        html = response.text\n",
    "        \n",
    "        #뷰티풀소프의 인자값 지정\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        #태그에서 제목과 링크주소 추출\n",
    "        atags = soup.select('._sp_each_title')\n",
    "        \n",
    "        title_text = []\n",
    "        link_text = []\n",
    "        source_text = []\n",
    "        date_text = []\n",
    "        contents_text = []\n",
    "        \n",
    "        for atag in atags:\n",
    "            title_text.append(atag.text) #제목\n",
    "            link_text.append(atag['href']) #링크주소\n",
    "\n",
    "        #신문사 추출\n",
    "        source_lists = soup.select('._sp_each_source')\n",
    "        for source_list in source_lists:\n",
    "            source_text.append(source_list.text) #신문사\n",
    "\n",
    "        #날짜 추출\n",
    "        date_lists = soup.select('.txt_inline')\n",
    "        for date_list in date_lists:\n",
    "            test = date_list.text\n",
    "            date_cleansing(test, date_text) #날짜 정제 함수사용\n",
    "            \n",
    "        #본문요약본\n",
    "        contents_lists = soup.select('ul.type01 dl')\n",
    "        for contents_list in contents_lists:\n",
    "            #print('==='*40)\n",
    "            #print(contents_list)\n",
    "            contents_cleansing(contents_list, contents_text) #본문요약 정제화\n",
    "            \n",
    "        #모든 리스트 딕셔너리형태로 저장\n",
    "        result= {\"date\" : date_text , \"title\":title_text , \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }\n",
    "        print(page)\n",
    "        \n",
    "        df = pd.DataFrame(result) #df로 변환\n",
    "        page += 10\n",
    "        \n",
    "        final_result = pd.concat([final_result, df], ignore_index=True)\n",
    "    return final_result\n",
    "#출처: https://bumcrush.tistory.com/116 [맑음때때로 여름]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜 정제화 함수\n",
    "def date_cleansing(test, date_text):\n",
    "    try:\n",
    "        #지난 뉴스\n",
    "        #머니투데이 10면1단 2018.11.05. 네이버뉴스 보내기\n",
    "        pattern = '\\d+.(\\d+).(\\d+).' #정규표현식\n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(0) # 2018.11.05.\n",
    "        date_text.append(match)\n",
    "    except AttributeError:\n",
    "        #최근 뉴스\n",
    "        #이데일리 1시간 전 네이버뉴스 보내기\n",
    "        pattern = '\\w* (\\d\\w*)' #정규표현식\n",
    "        r = re.compile(pattern)\n",
    "        match = r.search(test).group(1)\n",
    "        #print(match)\n",
    "        date_text.append(match)\n",
    "\n",
    "# 출처: https://bumcrush.tistory.com/116 [맑음때때로 여름]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#내용 정제화 함수\n",
    "def contents_cleansing(contents, contents_text):\n",
    "    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '',\n",
    "    str(contents)).strip() #앞에 필요없는 부분 제거\n",
    "    second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '',\n",
    "    first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "    contents_text.append(third_cleansing_contents)\n",
    "    #print(contents_text)\n",
    "\n",
    "# 출처: https://bumcrush.tistory.com/116 [맑음때때로 여름]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
